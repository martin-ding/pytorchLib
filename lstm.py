# -*- coding: utf-8 -*-
"""lstm

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GX0Rqur8T45MSYhLU9MYWAbycfLH4-Fu
"""
#
# !pip install torch
# !pip install torchtext
# !python -m spacy download en


# K80 gpu for 12 hours
import torch
from torch import nn, optim
from torchtext.data.utils import get_tokenizer
from torchtext import datasets
from torch.nn.utils.rnn import pad_sequence
from torchtext.vocab import GloVe, vocab, Vocab
from collections import Counter


# print('GPU:', torch.cuda.is_available())
SPECIAL_SYMBOLS = ['<unk>', '<pad>', '<bos>', '<eos>']
torch.manual_seed(123)
tokenizer = get_tokenizer('spacy',language= 'en_core_web_sm')
train_data, test_data = datasets.IMDB(split=('train', 'test'))

counter = Counter()
for (label, line) in train_data:
    counter.update(tokenizer(line))

vocab = vocab(counter, min_freq=10, specials=SPECIAL_SYMBOLS, )
vocab.set_default_index(0)

embedding_dim = 100
hidden_dim = 256

text_transform = lambda x: [vocab[token] for token in tokenizer(x)]
label_transform = lambda x: 1 if x == 2 else 0 # 1 neg 2 pos => pos 1 neg 0

# Print out the output of text_transform
# print("input to the text_transform:", "here is an example")
# print("output of the text_transform:", text_transform("here is an example"))

# word2vec, glove
# TEXT.build_vocab(train_data, max_size=10000, vectors='glove.6B.100d')
# LABEL.build_vocab(train_data)

batchsz = 30
def collate_batch(batch):
   label_list, text_list = [], []
   for (_label, _text) in batch:
        label_list.append(label_transform(_label))
        processed_text = torch.tensor(text_transform(_text))
        text_list.append(processed_text)
   return torch.tensor(label_list).type(torch.float32), pad_sequence(text_list)

train_dataloader = torch.utils.data.DataLoader(list(train_data), batchsz, shuffle=True,
                              collate_fn=collate_batch)

test_dataloader = torch.utils.data.DataLoader(list(test_data), batchsz, shuffle=True,
                              collate_fn=collate_batch)


device = torch.device('mps')

class RNN(nn.Module):
    
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        """
        """
        super(RNN, self).__init__()
        
        # [0-10001] => [100]
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        # [100] => [256]
        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, 
                           bidirectional=True, dropout=0.5)
        # [256*2] => [1]
        self.fc = nn.Linear(hidden_dim*2, 1)
        self.dropout = nn.Dropout(0.5)
        
        
    def forward(self, x):
        """
        x: [seq_len, b] vs [b, 3, 28, 28]
        """
        # [seq, b, 1] => [seq, b, 100]
        embedding = self.dropout(self.embedding(x))
        
        # output: [seq, b, hid_dim*2]
        # hidden/h: [num_layers*2, b, hid_dim]
        # cell/c: [num_layers*2, b, hid_di]
        output, (hidden, cell) = self.rnn(embedding)
        
        # [num_layers*2, b, hid_dim] => 2 of [b, hid_dim] => [b, hid_dim*2]
        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)
        
        # [b, hid_dim*2] => [b, 1]
        hidden = self.dropout(hidden)
        out = self.fc(hidden)
        
        return out

rnn = RNN(len(vocab), embedding_dim, hidden_dim)

golve = GloVe("6B", dim=embedding_dim)
pretrained_embedding = golve.get_vecs_by_tokens(vocab.get_itos())
print('pretrained_embedding:', pretrained_embedding.shape)
rnn.embedding.weight.data.copy_(pretrained_embedding)
print('embedding layer inited.')

optimizer = optim.Adam(rnn.parameters(), lr=1e-3)
criteon = nn.BCEWithLogitsLoss().to(device)
rnn.to(device)

import numpy as np

def binary_acc(preds, y):
    """
    get accuracy
    """
    preds = torch.round(torch.sigmoid(preds))
    correct = torch.eq(preds, y).float()
    acc = correct.sum() / len(correct)
    return acc

def train(rnn, iterator, optimizer, criteon):
    
    avg_acc = []
    rnn.train()
    
    for i, batch in enumerate(iterator):
        
        # [seq, b] => [b, 1] => [b]
        text = batch[1].to(device)
        label = batch[0].to(device)
        pred = rnn(text).squeeze(1)
        # 
        loss = criteon(pred, label)
        acc = binary_acc(pred, label).item()
        avg_acc.append(acc)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if i%10 == 0:
            print(i, acc)
        
    avg_acc = np.array(avg_acc).mean()
    print('avg acc:', avg_acc)
    
    
def eval(rnn, iterator, criteon):
    
    avg_acc = []
    
    rnn.eval()
    
    with torch.no_grad():
        for batch in iterator:
            text = batch[1].to(device)
            label = batch[0].to(device)
            # [b, 1] => [b]
            pred = rnn(text).squeeze(1)

            #
            # loss = criteon(pred, batch.label)

            acc = binary_acc(pred, label).item()
            avg_acc.append(acc)
        
    avg_acc = np.array(avg_acc).mean()
    
    print('>>test:', avg_acc)

for epoch in range(10):
    
    eval(rnn, test_dataloader, criteon)
    train(rnn, train_dataloader, optimizer, criteon)